{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Visual Property Norms results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move to root folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM, BertConfig, CLIPModel, CLIPProcessor, VisualBertForPreTraining, LxmertForPreTraining\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import copy\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "# TODO: make sure that paths work\n",
    "from models.src.clip_bert.modeling_bert import BertImageForMaskedLM\n",
    "from models.src.lxmert.alterations import LxmertLanguageOnlyXLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERIES_FOLDER = \"visual_property_norms/data/queries\"\n",
    "\n",
    "with open(\"visual_property_norms/data/labels.txt\", \"r\") as f:\n",
    "    MASK_LABELS = [label.strip() for label in f.readlines()]\n",
    "\n",
    "def get_model_preds_for_questions(model, tokenizer, questions):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    dataloader = DataLoader(questions, batch_size=64, shuffle=False)\n",
    "    with torch.no_grad():\n",
    "        preds = []\n",
    "        for questions_batch in iter(dataloader):\n",
    "            inputs = tokenizer(questions_batch, return_tensors=\"pt\", padding=True).to(device)\n",
    "            mask_idx = (inputs.input_ids == tokenizer.mask_token_id).nonzero(as_tuple=False)[:, 1][:, None]\n",
    "            assert len(mask_idx) == inputs.input_ids.shape[0], \"ERROR: Found multiple [MASK] tokens per example\"\n",
    "            \n",
    "            outputs = model(**inputs)[\"logits\"] if \"logits\" in model(**inputs) else model(**inputs)[\"prediction_logits\"]\n",
    "            pred = outputs.gather(1, mask_idx.repeat(1, outputs.shape[-1]).unsqueeze(1)).squeeze(1)\n",
    "            preds.append(pred)\n",
    "\n",
    "    preds = torch.cat(preds)\n",
    "    return preds\n",
    "\n",
    "def get_map_score_for_preds(labels, pred, tokenizer):\n",
    "    scores = []\n",
    "    assert pred[0].shape[0] == tokenizer.vocab_size\n",
    "    vocab = tokenizer.get_vocab()\n",
    "    for query_ix in range(len(labels)):\n",
    "        y_true = [0]*tokenizer.vocab_size\n",
    "        for label in labels[query_ix]:\n",
    "            y_true[vocab[label]] = 1 \n",
    "        scores.append(average_precision_score(y_true, pred[query_ix]))\n",
    "    \n",
    "    return scores\n",
    "\n",
    "def get_map_score_for_masked_preds(labels, pred, tokenizer, mask_labels):\n",
    "    scores = []\n",
    "    assert pred[0].shape[0] == tokenizer.vocab_size\n",
    "    vocab = tokenizer.get_vocab()\n",
    "    mask_ix = [vocab[mask_label] for mask_label in mask_labels]\n",
    "    \n",
    "    for query_ix in range(len(labels)):\n",
    "        y_true = [0]*len(mask_ix)\n",
    "        for label in labels[query_ix]:\n",
    "            y_true[mask_ix.index(vocab[label])] = 1 \n",
    "        scores.append(average_precision_score(y_true, pred[query_ix][mask_ix]))\n",
    "    \n",
    "    return scores\n",
    "\n",
    "def visualize_predictions(pred, questions, labels, tokenizer, num):\n",
    "    random_ix = np.random.choice(len(pred), num, replace=False)\n",
    "    for i in random_ix:\n",
    "        print(\"-------------------------------\")\n",
    "        print(f\"Question: {questions[i]}\")\n",
    "        print(f\"Golden labels: {labels[i]}\")\n",
    "        print(f\"Predicted labels: {tokenizer.decode(pred[i].topk(k=20).indices)}\")\n",
    "        print(\"-------------------------------\")\n",
    "        \n",
    "def update_results_with_model_preds(results, get_preds, query_files, tokenizer):\n",
    "    for query_file in tqdm(query_files):\n",
    "        with open(os.path.join(QUERIES_FOLDER, query_file)) as f:\n",
    "            examples = [json.loads(line) for line in f.readlines()]\n",
    "\n",
    "        questions = [ex[\"query\"] for ex in examples]\n",
    "        labels = [ex[\"labels\"] for ex in examples]\n",
    "        concepts = [ex[\"concept\"] for ex in examples]\n",
    "        feature_starters = [ex[\"feature_starter\"] for ex in examples]\n",
    "        pred = get_preds(questions)\n",
    "        scores = get_map_score_for_preds(labels, pred.cpu().detach().numpy(), tokenizer)\n",
    "        masked_scores = get_map_score_for_masked_preds(labels, pred.cpu().detach().numpy(), tokenizer, MASK_LABELS)    \n",
    "        mean_nbr_alternatives = np.mean([len(alternatives) for alternatives in labels])\n",
    "\n",
    "        query_template = examples[0][\"query_template\"] #same for the same file\n",
    "        pf = examples[0][\"pf\"]\n",
    "        for query_ix in range(len(labels)):\n",
    "            assert len(results[(results.model==model_name) & \n",
    "                               (results.concept==concepts[query_ix]) & \n",
    "                               (results.feature_starter==feature_starters[query_ix]) & \n",
    "                               (results.query_template==query_template) & \n",
    "                               (results.pf==pf)]) == 0, \"Should not append results to already existing key values\"\n",
    "            results_entry = {\"model\": model_name, \n",
    "                             \"concept\": concepts[query_ix],\n",
    "                             \"query_template\": query_template, \n",
    "                             \"feature_starter\": feature_starters[query_ix],\n",
    "                             \"pf\": pf,\n",
    "                             \"score\": scores[query_ix], \n",
    "                             \"masked_score\": masked_scores[query_ix], \n",
    "                             \"nbr_alternatives\": len(labels[query_ix]),\n",
    "                             \"top10_preds\": tokenizer.convert_ids_to_tokens(pred[query_ix].topk(k=10).indices),\n",
    "                             #\"top10_preds\": [print(val) for val in pred[query_ix].topk(k=10).indices],\n",
    "                             \"gold_labels\": labels[query_ix]}\n",
    "            results = results.append(results_entry, ignore_index=True).reset_index(drop=True)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation results will be added to this data frame\n",
    "results = pd.DataFrame(columns=[\"model\", \n",
    "                                \"concept\", \n",
    "                                \"query_template\", \n",
    "                                \"feature_starter\",\n",
    "                                \"pf\", \n",
    "                                \"score\", \n",
    "                                \"masked_score\", \n",
    "                                \"nbr_alternatives\", \n",
    "                                \"top10_preds\", \n",
    "                                \"gold_labels\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate BERT\n",
    "Measure with MAP. Report results per 1) pf split and 2) feature starter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 45/45 [36:49<00:00, 49.11s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "get_preds = lambda questions: get_model_preds_for_questions(model, tokenizer, questions)\n",
    "\n",
    "query_files = os.listdir(QUERIES_FOLDER)\n",
    "results = update_results_with_model_preds(results, get_preds, query_files, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_RESULTS_FILE = \"visual_property_norms/data/results-\"+model_name+\".csv\"\n",
    "save_results = True\n",
    "if save_results:\n",
    "    results.to_csv(MODEL_RESULTS_FILE, index=False)\n",
    "    results = pd.DataFrame(columns=[\"model\", \n",
    "                                \"concept\", \n",
    "                                \"query_template\", \n",
    "                                \"feature_starter\",\n",
    "                                \"pf\", \n",
    "                                \"score\", \n",
    "                                \"masked_score\", \n",
    "                                \"nbr_alternatives\", \n",
    "                                \"top10_preds\", \n",
    "                                \"gold_labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate bert finetuned on visual-text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 45/45 [37:00<00:00, 49.34s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-base-trained\"\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertImageForMaskedLM(config)\n",
    "model.load_state_dict(torch.load(\"models/data/model-weights/bert-clip-bert-trained/mp_rank_00_model_states.pt\", map_location=\"cpu\")[\"module\"], strict=False)\n",
    "model.eval()\n",
    "   \n",
    "get_preds = lambda questions: get_model_preds_for_questions(model, tokenizer, questions)\n",
    "\n",
    "query_files = os.listdir(QUERIES_FOLDER)\n",
    "results = update_results_with_model_preds(results, get_preds, query_files, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_RESULTS_FILE = \"visual_property_norms/data/results/results-\"+model_name+\".csv\"\n",
    "save_results = True\n",
    "if save_results:\n",
    "    results.to_csv(MODEL_RESULTS_FILE, index=False)\n",
    "    results = pd.DataFrame(columns=[\"model\", \n",
    "                                \"concept\", \n",
    "                                \"query_template\", \n",
    "                                \"feature_starter\",\n",
    "                                \"pf\", \n",
    "                                \"score\", \n",
    "                                \"masked_score\", \n",
    "                                \"nbr_alternatives\", \n",
    "                                \"top10_preds\", \n",
    "                                \"gold_labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate clip-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clip_bert_model(bert_image_model_path: str, no_visual_prediction: bool=False):\n",
    "    # Load BertImageForMaskedLM model\n",
    "    config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "    bert_image_model = BertImageForMaskedLM(config).eval()\n",
    "    bert_image_model.load_state_dict(torch.load(bert_image_model_path, map_location=\"cpu\")[\"module\"], strict=False)\n",
    "    bert_image_model.eval()\n",
    "\n",
    "    # Load CLIP\n",
    "    if not no_visual_prediction:\n",
    "        clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").eval()\n",
    "        clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "        return bert_image_model, clip_model, clip_processor\n",
    "    else:\n",
    "        return bert_image_model, None, None\n",
    "\n",
    "def get_clip_bert_preds_for_questions(model, \n",
    "                                      clip_model, \n",
    "                                      clip_processor,\n",
    "                                      questions,\n",
    "                                      tokenizer,\n",
    "                                      no_visual_prediction: bool=False):\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    clip_model = clip_model.to(device) if clip_model is not None else clip_model\n",
    "    \n",
    "    dataloader = DataLoader(questions, batch_size=64, shuffle=False)\n",
    "    with torch.no_grad():\n",
    "        preds = []\n",
    "        for questions_batch in iter(dataloader):\n",
    "            inputs = tokenizer(questions_batch, return_tensors=\"pt\", padding=True).to(device)\n",
    "            \n",
    "            # Predict visual features using CLIP\n",
    "            if not no_visual_prediction:\n",
    "                img_feats = clip_model.get_text_features(**clip_processor(text=questions_batch, return_tensors=\"pt\", padding=True).to(device)).unsqueeze(1)\n",
    "                inputs[\"img_feats\"] = img_feats\n",
    "                \n",
    "            outputs = model(**inputs)[\"logits\"]\n",
    "            \n",
    "            mask_idx = (inputs.input_ids == tokenizer.mask_token_id).nonzero(as_tuple=False)[:, 1][:, None]\n",
    "            assert len(mask_idx) == inputs.input_ids.shape[0], \"ERROR: Found multiple [MASK] tokens per example\"\n",
    "            pred = outputs.gather(1, mask_idx.repeat(1, outputs.shape[-1]).unsqueeze(1)).squeeze(1)\n",
    "            preds.append(pred)\n",
    "        preds = torch.cat(preds)\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 45/45 [36:59<00:00, 49.33s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"clip-bert-regress\"\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model_path = \"models/data/model-weights/clip-bert/mp_rank_00_model_states.pt\"\n",
    "\n",
    "model, clip_model, clip_processor = get_clip_bert_model(model_path, no_visual_prediction=False)\n",
    " \n",
    "get_preds = lambda questions: get_clip_bert_preds_for_questions(model, clip_model, clip_processor, questions, tokenizer, no_visual_prediction=False)\n",
    "\n",
    "query_files = os.listdir(QUERIES_FOLDER)\n",
    "results = update_results_with_model_preds(results, get_preds, query_files, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_RESULTS_FILE = \"visual_property_norms/data/results/results-\"+model_name+\".csv\"\n",
    "save_results = True\n",
    "if save_results:\n",
    "    results.to_csv(MODEL_RESULTS_FILE, index=False)\n",
    "    results = pd.DataFrame(columns=[\"model\", \n",
    "                                \"concept\", \n",
    "                                \"query_template\", \n",
    "                                \"feature_starter\",\n",
    "                                \"pf\", \n",
    "                                \"score\", \n",
    "                                \"masked_score\", \n",
    "                                \"nbr_alternatives\", \n",
    "                                \"top10_preds\", \n",
    "                                \"gold_labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate clip-bert without regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 45/45 [36:26<00:00, 48.60s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"clip-bert\"\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model_path = \"models/data/model-weights/clip-bert/mp_rank_00_model_states.pt\"\n",
    "\n",
    "model, clip_model, clip_processor = get_clip_bert_model(model_path, no_visual_prediction=True)\n",
    "\n",
    "get_preds = lambda questions: get_clip_bert_preds_for_questions(model, clip_model, clip_processor, questions, tokenizer, no_visual_prediction=True)\n",
    "\n",
    "query_files = os.listdir(QUERIES_FOLDER)\n",
    "results = update_results_with_model_preds(results, get_preds, query_files, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_RESULTS_FILE = \"visual_property_norms/data/results/results-\"+model_name+\".csv\"\n",
    "save_results = True\n",
    "if save_results:\n",
    "    results.to_csv(MODEL_RESULTS_FILE, index=False)\n",
    "    results = pd.DataFrame(columns=[\"model\", \n",
    "                                \"concept\", \n",
    "                                \"query_template\", \n",
    "                                \"feature_starter\",\n",
    "                                \"pf\", \n",
    "                                \"score\", \n",
    "                                \"masked_score\", \n",
    "                                \"nbr_alternatives\", \n",
    "                                \"top10_preds\", \n",
    "                                \"gold_labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate VisualBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 45/45 [37:09<00:00, 49.55s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"visualbert-vqa-coco\"\n",
    "model = VisualBertForPreTraining.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\").eval()\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "get_preds = lambda questions: get_model_preds_for_questions(model, tokenizer, questions)\n",
    "\n",
    "query_files = os.listdir(QUERIES_FOLDER)\n",
    "results = update_results_with_model_preds(results, get_preds, query_files, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_RESULTS_FILE = \"visual_property_norms/data/results/results-\"+model_name+\".csv\"\n",
    "save_results = True\n",
    "if save_results:\n",
    "    results.to_csv(MODEL_RESULTS_FILE, index=False)\n",
    "    results = pd.DataFrame(columns=[\"model\", \n",
    "                                \"concept\", \n",
    "                                \"query_template\", \n",
    "                                \"feature_starter\",\n",
    "                                \"pf\", \n",
    "                                \"score\", \n",
    "                                \"masked_score\", \n",
    "                                \"nbr_alternatives\", \n",
    "                                \"top10_preds\", \n",
    "                                \"gold_labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LXMERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_SHAPE = (1, 2048)\n",
    "NORMALIZED_BOXES_SHAPE = (1, 4)\n",
    "\n",
    "def get_lxmert_preds_for_questions(model, tokenizer, questions):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    dataloader = DataLoader(questions, batch_size=64, shuffle=False)\n",
    "    with torch.no_grad():\n",
    "        preds = []\n",
    "        for questions_batch in iter(dataloader):\n",
    "            inputs = tokenizer(questions_batch, return_tensors=\"pt\", padding=True).to(device)\n",
    "            mask_idx = (inputs.input_ids == tokenizer.mask_token_id).nonzero(as_tuple=False)[:, 1][:, None]\n",
    "            assert len(mask_idx) == inputs.input_ids.shape[0], \"ERROR: Found multiple [MASK] tokens per example\"\n",
    "            \n",
    "            nbr_samples = len(questions_batch)\n",
    "            normalized_boxes = torch.empty((nbr_samples,)+NORMALIZED_BOXES_SHAPE).uniform_(0, 1).to(device)\n",
    "            features = torch.empty((nbr_samples,)+FEATURES_SHAPE).uniform_(0, 10).to(device)\n",
    "            inputs.update({\n",
    "                \"visual_feats\": features,\n",
    "                \"visual_pos\": normalized_boxes\n",
    "            })\n",
    "            outputs = model(**inputs)[\"logits\"] if \"logits\" in model(**inputs) else model(**inputs)[\"prediction_logits\"]\n",
    "            pred = outputs.gather(1, mask_idx.repeat(1, outputs.shape[-1]).unsqueeze(1)).squeeze(1)\n",
    "            preds.append(pred)\n",
    "\n",
    "    preds = torch.cat(preds)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 45/45 [37:40<00:00, 50.23s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"lxmert-base-uncased\"\n",
    "model = LxmertForPreTraining.from_pretrained(\"unc-nlp/lxmert-base-uncased\")\n",
    "prev_encoder = copy.deepcopy(model.lxmert.encoder)\n",
    "model.lxmert.encoder.x_layers = torch.nn.ModuleList([LxmertLanguageOnlyXLayer(model.lxmert.encoder.config) for _ in range(model.lxmert.encoder.config.x_layers)])\n",
    "model.lxmert.encoder.load_state_dict(prev_encoder.state_dict())\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "get_preds = lambda questions: get_lxmert_preds_for_questions(model, tokenizer, questions)\n",
    "\n",
    "query_files = os.listdir(QUERIES_FOLDER)\n",
    "results = update_results_with_model_preds(results, get_preds, query_files, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_RESULTS_FILE = \"visual_property_norms/data/results/results-\"+model_name+\".csv\"\n",
    "save_results = True\n",
    "if save_results:\n",
    "    results.to_csv(MODEL_RESULTS_FILE, index=False)\n",
    "    results = pd.DataFrame(columns=[\"model\", \n",
    "                                \"concept\", \n",
    "                                \"query_template\", \n",
    "                                \"feature_starter\",\n",
    "                                \"pf\", \n",
    "                                \"score\", \n",
    "                                \"masked_score\", \n",
    "                                \"nbr_alternatives\", \n",
    "                                \"top10_preds\", \n",
    "                                \"gold_labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 45/45 [35:17<00:00, 47.06s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"random-baseline\"\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    " \n",
    "get_preds = lambda questions: torch.rand((len(questions), tokenizer.vocab_size))\n",
    "\n",
    "query_files = os.listdir(QUERIES_FOLDER)\n",
    "results = update_results_with_model_preds(results, get_preds, query_files, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_RESULTS_FILE = \"visual_property_norms/data/results/results-\"+model_name+\".csv\"\n",
    "save_results = True\n",
    "if save_results:\n",
    "    results.to_csv(MODEL_RESULTS_FILE, index=False)\n",
    "    results = pd.DataFrame(columns=[\"model\", \n",
    "                                \"concept\", \n",
    "                                \"query_template\", \n",
    "                                \"feature_starter\",\n",
    "                                \"pf\", \n",
    "                                \"score\", \n",
    "                                \"masked_score\", \n",
    "                                \"nbr_alternatives\", \n",
    "                                \"top10_preds\", \n",
    "                                \"gold_labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate BERT trained on LXMERT data from pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 45/45 [36:59<00:00, 49.32s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-base-trained-lxmert\"\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertImageForMaskedLM(config)\n",
    "model.load_state_dict(torch.load(\"models/data/model-weights/bert-lxmert-trained/mp_rank_00_model_states.pt\", map_location=\"cpu\")[\"module\"], strict=False)\n",
    "model.eval()\n",
    "   \n",
    "get_preds = lambda questions: get_model_preds_for_questions(model, tokenizer, questions)\n",
    "\n",
    "query_files = os.listdir(QUERIES_FOLDER)\n",
    "results = update_results_with_model_preds(results, get_preds, query_files, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_RESULTS_FILE = \"visual_property_norms/data/results/results-\"+model_name+\".csv\"\n",
    "save_results = True\n",
    "if save_results:\n",
    "    results.to_csv(MODEL_RESULTS_FILE, index=False)\n",
    "    results = pd.DataFrame(columns=[\"model\", \n",
    "                                \"concept\", \n",
    "                                \"query_template\", \n",
    "                                \"feature_starter\",\n",
    "                                \"pf\", \n",
    "                                \"score\", \n",
    "                                \"masked_score\", \n",
    "                                \"nbr_alternatives\", \n",
    "                                \"top10_preds\", \n",
    "                                \"gold_labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate BERT trained on LXMERT data from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 45/45 [37:58<00:00, 50.63s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-base-trained-lxmert-scratch\"\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertImageForMaskedLM(config)\n",
    "model.load_state_dict(torch.load(\"models/data/model-weights/bert-lxmert-trained-scratch/mp_rank_00_model_states.pt\", map_location=\"cpu\")[\"module\"], strict=False)\n",
    "model.eval()\n",
    "   \n",
    "get_preds = lambda questions: get_model_preds_for_questions(model, tokenizer, questions)\n",
    "\n",
    "query_files = os.listdir(QUERIES_FOLDER)\n",
    "results = update_results_with_model_preds(results, get_preds, query_files, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_RESULTS_FILE = \"visual_property_norms/data/results/results-\"+model_name+\".csv\"\n",
    "save_results = True\n",
    "if save_results:\n",
    "    results.to_csv(MODEL_RESULTS_FILE, index=False)\n",
    "    results = pd.DataFrame(columns=[\"model\", \n",
    "                                \"concept\", \n",
    "                                \"query_template\", \n",
    "                                \"feature_starter\",\n",
    "                                \"pf\", \n",
    "                                \"score\", \n",
    "                                \"masked_score\", \n",
    "                                \"nbr_alternatives\", \n",
    "                                \"top10_preds\", \n",
    "                                \"gold_labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\"bert-base-uncased\",\n",
    "               \"bert-base-trained\",\n",
    "               \"clip-bert-regress\",\n",
    "               \"clip-bert\",\n",
    "               \"visualbert-vqa-coco\",\n",
    "               \"lxmert-base-uncased\",\n",
    "               \"random-baseline\",\n",
    "               \"bert-base-trained-lxmert\",\n",
    "               \"bert-base-trained-lxmert-scratch\"]\n",
    "\n",
    "RESULTS_FOLDER = \"visual_property_norms/data/results/\"\n",
    "load_results = True\n",
    "\n",
    "if load_results:\n",
    "    results = pd.DataFrame(columns=[\"model\", \n",
    "                                \"concept\", \n",
    "                                \"query_template\", \n",
    "                                \"feature_starter\",\n",
    "                                \"pf\", \n",
    "                                \"score\", \n",
    "                                \"masked_score\", \n",
    "                                \"nbr_alternatives\", \n",
    "                                \"top10_preds\", \n",
    "                                \"gold_labels\"])\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        results_file = RESULTS_FOLDER+\"results-\"+model_name+\".csv\"\n",
    "        results = results.append(pd.read_csv(results_file), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results = False\n",
    "if save_results:\n",
    "    results.to_csv(\"visual_property_norms/data/results/results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
