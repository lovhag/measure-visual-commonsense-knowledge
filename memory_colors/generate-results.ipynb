{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f6b33b2",
   "metadata": {},
   "source": [
    "# Evaluate multimodal models on Memory Colors\n",
    "Currently VisualBERT and LXMERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f614c84",
   "metadata": {},
   "source": [
    "Move to the root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef5f6450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/lovhag/Projects/reproduce-visual-commonsense-eval\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17f4181c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM, BertConfig, VisualBertForPreTraining, LxmertForPreTraining, CLIPModel, CLIPProcessor\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import copy\n",
    "\n",
    "from models.src.clip_bert.modeling_bert import BertImageForMaskedLM\n",
    "from models.src.lxmert.alterations import LxmertLanguageOnlyXLayer\n",
    "from memory_colors.src.evaluate_bert import predict_masked, names_to_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b957a0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_preds_for_questions(model, tokenizer, questions):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    dataloader = DataLoader(questions, batch_size=64, shuffle=False)\n",
    "    with torch.no_grad():\n",
    "        preds = []\n",
    "        for questions_batch in iter(dataloader):\n",
    "            inputs = tokenizer(questions_batch, return_tensors=\"pt\", padding=True).to(device)\n",
    "            mask_idx = (inputs.input_ids == tokenizer.mask_token_id).nonzero(as_tuple=False)[:, 1][:, None]\n",
    "            assert len(mask_idx) == inputs.input_ids.shape[0], \"ERROR: Found multiple [MASK] tokens per example\"\n",
    "            \n",
    "            outputs = model(**inputs)[\"logits\"] if \"logits\" in model(**inputs) else model(**inputs)[\"prediction_logits\"]\n",
    "            pred = outputs.gather(1, mask_idx.repeat(1, outputs.shape[-1]).unsqueeze(1)).squeeze(1)\n",
    "            preds.append(pred)\n",
    "\n",
    "    preds = torch.cat(preds)\n",
    "    return preds\n",
    "\n",
    "def update_results_with_model_preds(results, get_preds, query_files, tokenizer):\n",
    "    for query_file in tqdm(query_files):\n",
    "        with open(os.path.join(QUERIES_FOLDER, query_file)) as f:\n",
    "            examples = [json.loads(line) for line in f.readlines()]\n",
    "\n",
    "        questions = [ex[\"query\"] for ex in examples]\n",
    "        labels = [ex[\"labels\"] for ex in examples]\n",
    "        pred = get_preds(questions)\n",
    "        score = get_map_score_for_preds(labels, pred.cpu().detach().numpy(), tokenizer)\n",
    "        masked_score = get_map_score_for_masked_preds(labels, pred.cpu().detach().numpy(), tokenizer, MASK_LABELS)    \n",
    "        support = len(questions)\n",
    "        mean_nbr_alternatives = np.mean([len(alternatives) for alternatives in labels])\n",
    "\n",
    "        query_type = query_file.split('.')[0]\n",
    "        assert len(results[(results.model==model_name) & (results.query_type==query_type)]) == 0, \"Should not append results to already existing key values\"\n",
    "        results = results.append({\"model\": model_name, \"query_type\": query_type, \"score\": score, \"masked_score\": masked_score, \"support\": support, \"mean_nbr_alternatives\": mean_nbr_alternatives}, ignore_index=True).reset_index(drop=True)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "614f36c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION_TEMPLATES = [\"Q: What is the color of [DESCRIPTOR] [ITEM]? A: It is [MASK].\",\n",
    "                      \"Q: What is the color of [DESCRIPTOR] [ITEM]? [SEP] A: It is [MASK].\",\n",
    "                      \"Q: What is the colour of [DESCRIPTOR] [ITEM]? A: It is [MASK].\",\n",
    "                      \"What is the color of [DESCRIPTOR] [ITEM]? [MASK].\",\n",
    "                      \"What is the color of [DESCRIPTOR] [ITEM]? [SEP] [MASK].\",\n",
    "                      \"What is the colour of [DESCRIPTOR] [ITEM]? [MASK].\",\n",
    "                      \"The color of [DESCRIPTOR] [ITEM] is [MASK].\",\n",
    "                      \"The usual color of [DESCRIPTOR] [ITEM] is [MASK].\",\n",
    "                      \"[DESCRIPTOR] [ITEM] usually has the color of [MASK].\",\n",
    "                      \"What is the usual color of [DESCRIPTOR] [ITEM]? [MASK].\",\n",
    "                      \"What is the usual color of [DESCRIPTOR] [ITEM]? [SEP] [MASK].\",\n",
    "                      \"What is the typical color of [DESCRIPTOR] [ITEM]? [MASK].\",\n",
    "                      \"What is the typical color of [DESCRIPTOR] [ITEM]? [SEP] [MASK].\"]\n",
    "\n",
    "OBJECT_COLORS_DATAFILE = \"memory_colors/data/memory_colors.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1edf3eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(OBJECT_COLORS_DATAFILE, \"r\") as f:\n",
    "    examples = [json.loads(line) for line in f.readlines()]\n",
    "    \n",
    "CLASS_NAMES = list(set(ex[\"label\"] for ex in examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d138fd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation results will be added to this data frame\n",
    "results = pd.DataFrame(columns=[\"model\", \"question_template\", \"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9aebc61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\").eval()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "class_names_ids = names_to_token_ids(CLASS_NAMES, tokenizer)\n",
    "for question_template in QUESTION_TEMPLATES:\n",
    "    questions = []\n",
    "    for ex in examples:\n",
    "        if ex[\"descriptor\"] == '':\n",
    "            question = question_template.replace(\"[DESCRIPTOR] \", \"\").replace(\"[ITEM]\", ex[\"item\"])\n",
    "        else:\n",
    "            question = question_template.replace(\"[DESCRIPTOR]\", ex[\"descriptor\"]).replace(\"[ITEM]\", ex[\"item\"])\n",
    "        questions.append(question)\n",
    "\n",
    "    inputs = tokenizer(questions, return_tensors=\"pt\", padding=True).to(device)\n",
    "    pred = predict_masked(inputs, tokenizer, model.to(device), class_names_ids).cpu().detach()\n",
    "    topk = pred.topk(1).indices # Rank color tokens and select top k (batch, k)\n",
    "    gt = torch.tensor([CLASS_NAMES.index(ex[\"label\"]) for ex in examples]).unsqueeze(1) # Index of ground truth color - (batch, 1)\n",
    "\n",
    "    precision_at_k = np.true_divide((topk == gt).sum(), gt.shape[0])\n",
    "    assert len(results[(results.model==model_name) & (results.question_template==question_template)]) == 0, \"Should not append results to already existing key values\"\n",
    "    results = results.append({\"model\": model_name, \"question_template\": question_template, \"accuracy\": float(precision_at_k.numpy())}, ignore_index=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d51bf331",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-trained\"\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertImageForMaskedLM(config)\n",
    "model.load_state_dict(torch.load(\"models/data/model-weights/bert-clip-bert-trained/mp_rank_00_model_states.pt\", map_location=\"cpu\")[\"module\"], strict=False)\n",
    "model.eval()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "class_names_ids = names_to_token_ids(CLASS_NAMES, tokenizer)\n",
    "for question_template in QUESTION_TEMPLATES:\n",
    "    questions = []\n",
    "    for ex in examples:\n",
    "        if ex[\"descriptor\"] == '':\n",
    "            question = question_template.replace(\"[DESCRIPTOR] \", \"\").replace(\"[ITEM]\", ex[\"item\"])\n",
    "        else:\n",
    "            question = question_template.replace(\"[DESCRIPTOR]\", ex[\"descriptor\"]).replace(\"[ITEM]\", ex[\"item\"])\n",
    "        questions.append(question)\n",
    "\n",
    "    inputs = tokenizer(questions, return_tensors=\"pt\", padding=True).to(device)\n",
    "    pred = predict_masked(inputs, tokenizer, model.to(device), class_names_ids).cpu().detach()\n",
    "    topk = pred.topk(1).indices # Rank color tokens and select top k (batch, k)\n",
    "    gt = torch.tensor([CLASS_NAMES.index(ex[\"label\"]) for ex in examples]).unsqueeze(1) # Index of ground truth color - (batch, 1)\n",
    "\n",
    "    precision_at_k = np.true_divide((topk == gt).sum(), gt.shape[0])\n",
    "    assert len(results[(results.model==model_name) & (results.question_template==question_template)]) == 0, \"Should not append results to already existing key values\"\n",
    "    results = results.append({\"model\": model_name, \"question_template\": question_template, \"accuracy\": float(precision_at_k.numpy())}, ignore_index=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a046f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"clip-bert-implicit\"\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertImageForMaskedLM(config)\n",
    "model.load_state_dict(torch.load(\"models/data/model-weights/clip-bert/mp_rank_00_model_states.pt\", map_location=\"cpu\")[\"module\"], strict=False)\n",
    "model.eval()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "class_names_ids = names_to_token_ids(CLASS_NAMES, tokenizer)\n",
    "for question_template in QUESTION_TEMPLATES:\n",
    "    questions = []\n",
    "    for ex in examples:\n",
    "        if ex[\"descriptor\"] == '':\n",
    "            question = question_template.replace(\"[DESCRIPTOR] \", \"\").replace(\"[ITEM]\", ex[\"item\"])\n",
    "        else:\n",
    "            question = question_template.replace(\"[DESCRIPTOR]\", ex[\"descriptor\"]).replace(\"[ITEM]\", ex[\"item\"])\n",
    "        questions.append(question)\n",
    "\n",
    "    inputs = tokenizer(questions, return_tensors=\"pt\", padding=True).to(device)\n",
    "    pred = predict_masked(inputs, tokenizer, model.to(device), class_names_ids).cpu().detach()\n",
    "    topk = pred.topk(1).indices # Rank color tokens and select top k (batch, k)\n",
    "    gt = torch.tensor([CLASS_NAMES.index(ex[\"label\"]) for ex in examples]).unsqueeze(1) # Index of ground truth color - (batch, 1)\n",
    "\n",
    "    precision_at_k = np.true_divide((topk == gt).sum(), gt.shape[0])\n",
    "    assert len(results[(results.model==model_name) & (results.question_template==question_template)]) == 0, \"Should not append results to already existing key values\"\n",
    "    results = results.append({\"model\": model_name, \"question_template\": question_template, \"accuracy\": float(precision_at_k.numpy())}, ignore_index=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d10e5811",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"clip-bert-explicit\"\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertImageForMaskedLM(config)\n",
    "model.load_state_dict(torch.load(\"models/data/model-weights/clip-bert/mp_rank_00_model_states.pt\", map_location=\"cpu\")[\"module\"], strict=False)\n",
    "model.eval()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "class_names_ids = names_to_token_ids(CLASS_NAMES, tokenizer)\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").eval()\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        \n",
    "for question_template in QUESTION_TEMPLATES:\n",
    "    questions = []\n",
    "    for ex in examples:\n",
    "        if ex[\"descriptor\"] == '':\n",
    "            question = question_template.replace(\"[DESCRIPTOR] \", \"\").replace(\"[ITEM]\", ex[\"item\"])\n",
    "        else:\n",
    "            question = question_template.replace(\"[DESCRIPTOR]\", ex[\"descriptor\"]).replace(\"[ITEM]\", ex[\"item\"])\n",
    "        questions.append(question)\n",
    "\n",
    "    inputs = tokenizer(questions, return_tensors=\"pt\", padding=True)\n",
    "    img_feats = clip_model.get_text_features(**clip_processor(text=questions, return_tensors=\"pt\", padding=True)).unsqueeze(1)\n",
    "    inputs[\"img_feats\"] = img_feats\n",
    "    \n",
    "    pred = predict_masked(inputs.to(device), tokenizer, model.to(device), class_names_ids).cpu().detach()\n",
    "    topk = pred.topk(1).indices # Rank color tokens and select top k (batch, k)\n",
    "    gt = torch.tensor([CLASS_NAMES.index(ex[\"label\"]) for ex in examples]).unsqueeze(1) # Index of ground truth color - (batch, 1)\n",
    "\n",
    "    precision_at_k = np.true_divide((topk == gt).sum(), gt.shape[0])\n",
    "    assert len(results[(results.model==model_name) & (results.question_template==question_template)]) == 0, \"Should not append results to already existing key values\"\n",
    "    results = results.append({\"model\": model_name, \"question_template\": question_template, \"accuracy\": float(precision_at_k.numpy())}, ignore_index=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dae825be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-trained-lxmert\"\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertImageForMaskedLM(config)\n",
    "model.load_state_dict(torch.load(\"models/data/model-weights/bert-lxmert-trained/mp_rank_00_model_states.pt\", map_location=\"cpu\")[\"module\"], strict=False)\n",
    "model.eval()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "class_names_ids = names_to_token_ids(CLASS_NAMES, tokenizer)\n",
    "for question_template in QUESTION_TEMPLATES:\n",
    "    questions = []\n",
    "    for ex in examples:\n",
    "        if ex[\"descriptor\"] == '':\n",
    "            question = question_template.replace(\"[DESCRIPTOR] \", \"\").replace(\"[ITEM]\", ex[\"item\"])\n",
    "        else:\n",
    "            question = question_template.replace(\"[DESCRIPTOR]\", ex[\"descriptor\"]).replace(\"[ITEM]\", ex[\"item\"])\n",
    "        questions.append(question)\n",
    "\n",
    "    inputs = tokenizer(questions, return_tensors=\"pt\", padding=True).to(device)\n",
    "    pred = predict_masked(inputs, tokenizer, model.to(device), class_names_ids).cpu().detach()\n",
    "    topk = pred.topk(1).indices # Rank color tokens and select top k (batch, k)\n",
    "    gt = torch.tensor([CLASS_NAMES.index(ex[\"label\"]) for ex in examples]).unsqueeze(1) # Index of ground truth color - (batch, 1)\n",
    "\n",
    "    precision_at_k = np.true_divide((topk == gt).sum(), gt.shape[0])\n",
    "    assert len(results[(results.model==model_name) & (results.question_template==question_template)]) == 0, \"Should not append results to already existing key values\"\n",
    "    results = results.append({\"model\": model_name, \"question_template\": question_template, \"accuracy\": float(precision_at_k.numpy())}, ignore_index=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd93e7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-trained-lxmert-scratch\"\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertImageForMaskedLM(config)\n",
    "model.load_state_dict(torch.load(\"models/data/model-weights/bert-lxmert-trained-scratch/mp_rank_00_model_states.pt\", map_location=\"cpu\")[\"module\"], strict=False)\n",
    "model.eval()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "class_names_ids = names_to_token_ids(CLASS_NAMES, tokenizer)\n",
    "for question_template in QUESTION_TEMPLATES:\n",
    "    questions = []\n",
    "    for ex in examples:\n",
    "        if ex[\"descriptor\"] == '':\n",
    "            question = question_template.replace(\"[DESCRIPTOR] \", \"\").replace(\"[ITEM]\", ex[\"item\"])\n",
    "        else:\n",
    "            question = question_template.replace(\"[DESCRIPTOR]\", ex[\"descriptor\"]).replace(\"[ITEM]\", ex[\"item\"])\n",
    "        questions.append(question)\n",
    "\n",
    "    inputs = tokenizer(questions, return_tensors=\"pt\", padding=True).to(device)\n",
    "    pred = predict_masked(inputs, tokenizer, model.to(device), class_names_ids).cpu().detach()\n",
    "    topk = pred.topk(1).indices # Rank color tokens and select top k (batch, k)\n",
    "    gt = torch.tensor([CLASS_NAMES.index(ex[\"label\"]) for ex in examples]).unsqueeze(1) # Index of ground truth color - (batch, 1)\n",
    "\n",
    "    precision_at_k = np.true_divide((topk == gt).sum(), gt.shape[0])\n",
    "    assert len(results[(results.model==model_name) & (results.question_template==question_template)]) == 0, \"Should not append results to already existing key values\"\n",
    "    results = results.append({\"model\": model_name, \"question_template\": question_template, \"accuracy\": float(precision_at_k.numpy())}, ignore_index=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd349430",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 631/631 [00:00<00:00, 125kB/s]\n",
      "Downloading: 100%|██████████| 428M/428M [00:35<00:00, 12.8MB/s] \n"
     ]
    }
   ],
   "source": [
    "model_name = \"visualbert-vqa-coco\"\n",
    "model = VisualBertForPreTraining.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\").eval()\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "class_names_ids = names_to_token_ids(CLASS_NAMES, tokenizer)\n",
    "for question_template in QUESTION_TEMPLATES:\n",
    "    questions = []\n",
    "    for ex in examples:\n",
    "        if ex[\"descriptor\"] == '':\n",
    "            question = question_template.replace(\"[DESCRIPTOR] \", \"\").replace(\"[ITEM]\", ex[\"item\"])\n",
    "        else:\n",
    "            question = question_template.replace(\"[DESCRIPTOR]\", ex[\"descriptor\"]).replace(\"[ITEM]\", ex[\"item\"])\n",
    "        questions.append(question)\n",
    "\n",
    "    inputs = tokenizer(questions, return_tensors=\"pt\", padding=True).to(device)\n",
    "    pred = predict_masked(inputs, tokenizer, model.to(device), class_names_ids).cpu().detach()\n",
    "    topk = pred.topk(1).indices # Rank color tokens and select top k (batch, k)\n",
    "    gt = torch.tensor([CLASS_NAMES.index(ex[\"label\"]) for ex in examples]).unsqueeze(1) # Index of ground truth color - (batch, 1)\n",
    "\n",
    "    precision_at_k = np.true_divide((topk == gt).sum(), gt.shape[0])\n",
    "    assert len(results[(results.model==model_name) & (results.question_template==question_template)]) == 0, \"Should not append results to already existing key values\"\n",
    "    results = results.append({\"model\": model_name, \"question_template\": question_template, \"accuracy\": float(precision_at_k.numpy())}, ignore_index=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94fa07dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"lxmert-base-uncased\"\n",
    "model = LxmertForPreTraining.from_pretrained(\"unc-nlp/lxmert-base-uncased\")\n",
    "prev_encoder = copy.deepcopy(model.lxmert.encoder)\n",
    "model.lxmert.encoder.x_layers = torch.nn.ModuleList([LxmertLanguageOnlyXLayer(model.lxmert.encoder.config) for _ in range(model.lxmert.encoder.config.x_layers)])\n",
    "model.lxmert.encoder.load_state_dict(prev_encoder.state_dict())\n",
    "model.eval()\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "FEATURES_SHAPE = (1, 2048)\n",
    "NORMALIZED_BOXES_SHAPE = (1, 4)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "class_names_ids = names_to_token_ids(CLASS_NAMES, tokenizer)\n",
    "for question_template in QUESTION_TEMPLATES:\n",
    "    questions = []\n",
    "    for ex in examples:\n",
    "        if ex[\"descriptor\"] == '':\n",
    "            question = question_template.replace(\"[DESCRIPTOR] \", \"\").replace(\"[ITEM]\", ex[\"item\"])\n",
    "        else:\n",
    "            question = question_template.replace(\"[DESCRIPTOR]\", ex[\"descriptor\"]).replace(\"[ITEM]\", ex[\"item\"])\n",
    "        questions.append(question)\n",
    "\n",
    "    inputs = tokenizer(questions, return_tensors=\"pt\", padding=True).to(device)\n",
    "    nbr_samples = len(questions)\n",
    "    normalized_boxes = torch.empty((nbr_samples,)+NORMALIZED_BOXES_SHAPE).uniform_(0, 1).to(device)\n",
    "    features = torch.empty((nbr_samples,)+FEATURES_SHAPE).uniform_(0, 10).to(device)\n",
    "    inputs.update({\n",
    "        \"visual_feats\": features,\n",
    "        \"visual_pos\": normalized_boxes\n",
    "    })\n",
    "            \n",
    "    pred = predict_masked(inputs, tokenizer, model.to(device), class_names_ids).cpu().detach()\n",
    "    topk = pred.topk(1).indices # Rank color tokens and select top k (batch, k)\n",
    "    gt = torch.tensor([CLASS_NAMES.index(ex[\"label\"]) for ex in examples]).unsqueeze(1) # Index of ground truth color - (batch, 1)\n",
    "\n",
    "    precision_at_k = np.true_divide((topk == gt).sum(), gt.shape[0])\n",
    "    assert len(results[(results.model==model_name) & (results.question_template==question_template)]) == 0, \"Should not append results to already existing key values\"\n",
    "    results = results.append({\"model\": model_name, \"question_template\": question_template, \"accuracy\": float(precision_at_k.numpy())}, ignore_index=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89aa66a",
   "metadata": {},
   "source": [
    "## Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5afe560c",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_FILE = \"memory_colors/data/results.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "48a020a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results = True\n",
    "if save_results:\n",
    "    results.to_csv(RESULTS_FILE, index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0f5770ad77efe7ab82fb28541812678c7782e8c1b9375e05d9330e0e75c1f82d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
